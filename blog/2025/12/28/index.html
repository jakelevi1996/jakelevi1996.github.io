<!DOCTYPE html>

<!-- LICENSE: https://github.com/jakelevi1996/jakelevi1996.github.io/blob/main/LICENSE -->

<html lang="en-GB">

<head>
    <meta charset="UTF-8">
    <meta name="viewport"           content="width=device-width, initial-scale=1.0">

    <meta property="og:title"       content="Future of AI"/>
    <meta property="og:type"        content="website"/>
    <meta property="og:image"       content="/img/og_image.png"/>
    <meta property="og:url"         content="https://jakelevi1996.github.io/"/>
    <meta property="og:description" content="Is the future of AI in good hands? An analysis of 3 Tweets"/>

    <title>Future of AI</title>
    <link rel="icon"                href="/img/favicon.png"  type="image/png"/>
    <link rel="apple-touch-icon"    href="/img/icon_144.png" type="image/png"/>
    <link rel="stylesheet"          href="/styles.css"/>

    <!-- From analytics.google.com -->
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-21V7C3ZYH0"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-21V7C3ZYH0');
    </script>
</head>

<body>
    <h1><a href="#">Future of AI</a></h1>

    <p><em>Is the future of AI in good hands? An analysis of 3 Tweets &#183; Jake Levi &#183; 2025-12-28 &#183; [&nbsp;<a href="/">Back to home</a>&nbsp;]</em></p>

    <p>New year, new me, new blog post (and new paper, which I have written, and will hopefully soon be able to publicise).</p>

    <p><em>[Preface: this blog post may sound pessimistic. I don't want to sound pessimistic, and I don't think it is good to be pessimistic. But I also think it's important to speak openly about distant problems on the horizon. Discussing such problems early gives us more chance to address them.]</em></p>

    <h2 id="intro"><a href="#intro">Introduction</a></h2>

    <p>Any good blog post or research paper should start by clarifying (i) what question we are answering, and (ii) why it is important. Hopefully the question is clear, otherwise try reading the title again. This question is important because it's a tractable component of a bigger and more important question: is AI progress going to have a positive impact on society? This bigger question could not be more important for me as an AI researcher. No matter how many citations I get, or how much I get paid, I do not want to do work that has a negative impact on society. Regardless of what anyone says on Twitter or LinkedIn in order to inflate the valuation of their company or brand, I don't think it should be taken for granted that AI is necessarily guaranteed to have a positive impact on society.</p>

    <p>This touches on an important point: there is no incentive for AI leaders in industry or academia to come out and openly say they think AI will have a negative impact on society, even if that is truly what they think. Does this mean that, by listening to such leaders, we can only hope to hear messages that are biased towards the positive potential impacts of AI, while conveniently skipping over the negatives? Fortunately, if you spend some time every now and then scrolling Twitter and pretending it counts as work, you see the occasional Tweet that reveals more than what appears to have been intended. In this blog post, I'm going to discuss 3 such examples, and give my unsolicited take on what I think we can learn from them.</p>

    <h2 id="1"><a href="#1">Tweet #1</a></h2>

    <p>Our first Tweet comes from Richard Sutton, who made pioneering contributions to an area of AI called "Reinforcement Learning". He is perhaps most widely known for co-authoring the book "Reinforcement Learning: An Introduction", published in 1998 and updated and rereleased in 2018. Here is the Tweet:</p>

    <blockquote class="twitter-tweet"><p lang="en" dir="ltr">AI researchers seek to understand intelligence well enough to create beings of greater intelligence than current humans. <br><br>Reaching this profound intellectual milestone will enrich our economies and challenge our societal institutions. It will be unprecedented and…</p>&mdash; Richard Sutton (@RichardSSutton) <a href="https://twitter.com/RichardSSutton/status/1681822942546759681?ref_src=twsrc%5Etfw">July 20, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

    <p>Let's try to trace out the logic here. Humans have been creating technology for thousands of years. Technology has historically been good for society. AI is a technology. Therefore AI will also be good for society. QED.</p>

    <p>The difference between previous technology and AI is that, historically, technology always more efficiently replaced SOME specific capability of humans or pre-existing technology, but not ALL capabilities. This leaves (or creates) a gap that can be filled by humans who have been displaced by newly created technology. The problem with AI is that the stated goal of many big tech companies is AGI ("artificial general intelligence"), which in basic English means to replace ALL capabilities of human intelligence. The problem is that if frontier AI labs are successful in solving AGI, that's it, that's the end-game. There is no gap left for displaced humans to fill. AI does everything. And all those new "opportunities" unlocked by the power of our new technology? Yeah, AI does those as well.</p>

    <p>This is a fundamental difference. It's the reason why we can't lump AI in the same category as the technology that came before it (and enabled it).</p>

    <p>Maybe that's amazing. Nobody ever has to work again. But there's a problem here too: power that was once held by humans (in the sense that humans are workers, and bosses DEPEND on workers to complete tasks) now becomes concentrated in big tech companies, whose AI models have replaced all the workers. So now we are relying on the benevolence of big tech companies to share the fruits of their labour evenly throughout society. And do we want our society to become fully and permanently dependent on the benevolence of big tech companies? [Question left as an exercise for the reader.]</p>

    <h2 id="2"><a href="#2">Tweet #2</a></h2>

    <p>Our next Tweet comes from Sam Altman, former and current (lol) CEO of OpenAI, the company that brought us ChatGPT. The context to this Tweet is someone pointing out that Sam owns zero shares in OpenAI, and expressing confusion about his motives. Here is Sam's reply:</p>

    <blockquote class="twitter-tweet"><p lang="en" dir="ltr">if i were like, a sports star or an artist or something, and just really cared about doing a great job at my thing, and was up at 5 am practicing free throws or whatever, that would seem pretty normal right?<br><br>the first part of openai was unbelievably fun; we did what i believe is…</p>&mdash; Sam Altman (@sama) <a href="https://twitter.com/sama/status/1983941806393024762?ref_src=twsrc%5Etfw">October 30, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

    <p>Sam is doing something quite interesting with language here. If you don't immediately see it, let me demonstrate with an example: "My two passions in life are saving the lives of cows, and eating delicious cheeseburgers". If you read that sentence while half asleep, it might not stand out to you. When we hear a sentence like "I care deeply about A and B", based on the statistics of language that we experience throughout our lives, it is highly likely that A and B are in some way aligned with each other, or at least are not inconsistent. We don't hear many such examples in which A and B are inconsistent, and so if someone slips in an example in which they are inconsistent, we may be forgiven for not noticing.</p>

    <p>In the case of Sam's Tweet, A and B are (A) getting "the chance to really "make a dent in the universe"" and (B) doing work which "will be a transformatively positive thing". In fact, the idea of wanting to "make a dent in the universe" is connected to the research of psychologist Alison Gopnik and others on the theory of "Empowerment". This theory proposes that infants and children learn to interact with the world in order to maximise "the mutual information between actions and their outcomes", or in plain English, to have "more wide-ranging control of their environment". Becoming the boss of a big tech company is a great way to gain wide-ranging control over your environment, and subsequently make a big dent in it.</p>

    <p>The problem is that Sam's A and B are not necessarily aligned, despite their linguistic framing! For example, suppose that the most "transformatively positive thing" that Sam could do for the future of society would be to fire all of OpenAI's employees, dissolve the company, and throw all their GPUs in the sea. In our hypothetical situation, this maximises objective B, but it significantly reduces Sam's opportunity to make any future progress on objective A, which is to make a "dent in the universe". So the question is, when Sam's two objectives do come into conflict, which will take priority? I think I can guess.</p>

    <h2 id="3"><a href="#3">Tweet #3</a></h2>

    <p>Tweet #3 comes from Andrew Ng, adjunct professor of computer science at Stanford University, and co-founder of both Google Brain (2011, now part of Google DeepMind) and Coursera (2012). His Tweet discusses a recent survey indicating that a large proportion of Americans do not trust AI:</p>

    <blockquote class="twitter-tweet"><p lang="en" dir="ltr">Separate reports by the publicity firm Edelman and Pew Research show that Americans, and more broadly large parts of Europe and the western world, do not trust AI and are not excited about it. (Links in original text, below.) Despite the AI community’s optimism about the…</p>&mdash; Andrew Ng (@AndrewYNg) <a href="https://twitter.com/AndrewYNg/status/1996631366470132053?ref_src=twsrc%5Etfw">December 4, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

    <p>The sensation I have reading the first paragraph of Andrew's Tweet is like the sensation I have when being <a href="https://upload.wikimedia.org/wikipedia/en/7/73/Trollface.png">trolled</a>. The paragraph's first sentence describes the survey data. The second sentence continues, referring to public distrust of AI, "we should take this seriously and not dismiss it". This response is promising, but open to interpretation. What does "take this seriously" mean in practice? Andrew clarifies his meaning and concludes the paragraph by saying "The public's concerns about AI can be a significant drag on progress, and we can do a lot to address them". Or in other words, "The public are weighing us down and we need to persuade them they're wrong".</p>

    <p>In a sense, Andrew is making the same mistake that can make LLMs (ChatGPT etc) so frustrating to use. The problem with LLMs (especially earlier versions) is that they don't question their own validity. Like many of us, LLMs occasionally make mistakes or misremember facts (referred to in AI literature as "hallucinating") part way through their reasoning. As humans, we have a remarkable capacity to be aware of our momentary uncertainty, and carry this forward to greater uncertainty in our final answers, or take action to address our uncertainty. In contrast, LLMs seem to lack this subtle ability. After a mistake, an LLM will continue on cheerfully without a care in the world, continuing to sound plausible, while carrying all their errors forward. The consequence is that LLMs have a tendency to produce answers which are not only wrong, but are confidently wrong.</p>

    <p>How does this relate to Andrew's Tweet? Well, an AI researcher could respond to hearing that 49% of surveyed Americans "reject the growing use of AI" in many different ways. The holistic response would be to appreciate that we as AI researchers are part of society. Especially those of us privileged enough to have done research at a university, funded by taxpayers, should feel a duty to do research that improves our society, or at the very least, does not make it worse. Hearing that 49% of surveyed Americans "reject the growing use of AI" should be a wake-up call that stimulates us as AI researchers to pause for a moment and question the validity of our research. Not the scientific validity, or the technical validity, but rather, what we might call the "social validity", the likely effects that our research will have on society. Unfortunately, much like an LLM, Andrew does not respond by questioning his validity, and instead dismisses public concern as a "significant drag on progress". I feel ashamed to call myself an AI researcher, if this is how AI leaders describe public concern.</p>

    <p>In fairness to Andrew, he does go on to discuss some legitimate concerns people do have about AI. But the solutions that Andrew offers to these problems are simply (1) for AI researchers and the media to stop spreading "hype", and (2) for people to learn more about AI using services such as DeepLearning.AI. Coincidentally, DeepLearning.AI only costs $25 per month for a membership. And whose smiling face greets you on the homepage of DeepLearning.AI? That's right, Andrew Ng.</p>

    <p>Andrew's whole Tweet is based on the assumption that AI will necessarily be a "huge national advantage", and that public concern is an inconvenience that needs to be fixed. Instead of asking "how can we convince the public that they're wrong about AI", I think that we as AI researchers need to consider the possibility that maybe the public are right about AI. Maybe we should be the ones questioning our values and validity, and rethinking the direction that AI research is pointing towards. While doing so, it's important to remember that whether or not AI is beneficial for society is about a lot more than just GDP. If GDP is doubled and wealth inequality is quadrupled, the effect for most people will likely be negative.</p>

    <h2 id="conclusions"><a href="#conclusions">Conclusions</a></h2>

    <p>Let's summarise. Richard Sutton thinks that AI research is justified simply because it is a continuation of existing trends. But this ignores the fundamental difference between AI and the technology that came before it, which is that AI doesn't leave gaps for displaced human workers to fill, and may lead to huge concentration of power within big tech companies. Sam Altman gave us some clues about what motivates him as a big tech CEO. But his motivations are not necessarily aligned with each other, and anyone can make their own guess about which will take priority when they come into conflict. Andrew Ng acknowledged that there is large public distrust of AI. But his dismissal of this distrust as "a drag on process" reveals his assumption that the public are wrong not to embrace AI, whereas arguably this assumption needs to be reexamined.</p>

    <p>The original question I asked was "Is the future of AI in good hands?". Based on this small but influential sample of hands, I am pessimistic about the answer to this question. I also mentioned that this was a component of a larger question, "is AI progress going to have a positive impact on society?". This is a difficult, multi-faceted question. As an analogy, imagine it's February 2004, Facebook has just been released, and someone asks you whether you think social media will have a positive impact on society. I think at that moment in time, it would have been easy to recognise the immediate positive impacts that social media would have on a personal level. I think it would have been very difficult to imagine the long-term negative societal impacts that would be emerging 20 years later. I worry that we are in a similar position today with AI.</p>

    <p>If AI is going to have negative impacts on society, then the most important questions we can ask are "What can we do about it? What should we do about it?". These are extremely difficult questions, arguably more difficult than solving AGI itself. One unfortunate reality we have to contend with is the force of incentives. There is a deep inertia to the growing competition between America and China. Neither side wants to fall behind and become disempowered. Even if a bunch of people wave their arms around shouting "Stop doing AI, it's bad!", even if they make the most convincing possible moral argument against AI, there will still be an incentive for people to ignore that argument and keep driving forward "progress".</p>

    <p>I'd like to end on a speculatively optimistic note. Just because technology can automate some process more efficiently, it does not mean that the original process loses its value. The analogy I like to make is to handmade pottery. It's more expensive to make pottery by hand compared to a production line, but people still value handmade pottery because it has a personal touch, and because of its imperfections. I predict that as time goes on, society will value human intelligence less and less, because even the smartest human will be a dumbass in every way compared to AI systems of the future. As this happens, I hope that our values will shift more towards the personal touch in human-to-human interactions.</p>

    <hr>

    <table>
        <tr>
            <td><a href="https://github.com/jakelevi1996/jakelevi1996.github.io">Source</a></td>
            <td><a href="https://github.com/jakelevi1996/jakelevi1996.github.io/blob/main/LICENSE">License</a></td>
        </tr>
    </table>

</body>

</html>
